---
url: https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677
title: Distributed Tracing Infrastructure with Jaeger on Kubernetes
archived_at: 2021-12-13T16:09:28.000277+08:00
---
<div id="readability-page-1" class="page"><div><p id="fd24">The following sections goes over enabling distributed tracing with Jaeger for gRPC services in a Kubernetes setup. Jaeger Github Org has <a href="https://github.com/jaegertracing/jaeger-kubernetes" rel="noopener ugc nofollow" target="_blank">dedicated repo</a> for various deployment configurations of Jaeger in Kubernetes. These are excellent examples to build on, and I will try to break down each Jaeger component and its Kubernetes deployment.</p><p id="2048">Jaeger is an open-source distributed tracing system that implements the OpenTracing specification. Jaeger includes components to store, visualize and filter traces.</p><h2 id="a412">Architecture</h2><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/1*I-JUG4eJhOTyrcs7teQwLg.png" width="700" height="376" srcset="https://miro.medium.com/max/552/1*I-JUG4eJhOTyrcs7teQwLg.png 276w, https://miro.medium.com/max/1104/1*I-JUG4eJhOTyrcs7teQwLg.png 552w, https://miro.medium.com/max/1280/1*I-JUG4eJhOTyrcs7teQwLg.png 640w, https://miro.medium.com/max/1400/1*I-JUG4eJhOTyrcs7teQwLg.png 700w" sizes="700px" role="presentation" data-old-src="https://miro.medium.com/max/60/1*I-JUG4eJhOTyrcs7teQwLg.png?q=20"/></p></div></figure><h2 id="8048">Jaeger Client</h2><p id="855d">Application tracing instrumentation starts with the Jaeger client. The following example uses the Jaeger Go library to initialize tracer configuration from environment variables and enable <a href="https://github.com/jaegertracing/jaeger-client-go/blob/master/metrics.go" rel="noopener ugc nofollow" target="_blank">client-side metrics</a>.</p><figure></figure><p id="6928">The Go client makes it simple to initialize the Jaeger configuration via environment variables. Some of the important environment variables to set include <code>JAEGER_SERVICE_NAME</code>, <code>JAEGER_AGENT_HOST</code> and <code>JAEGER_AGENT_PORT</code>. Full list of environment variables supported by Jaeger Go Client is listed <a href="https://github.com/jaegertracing/jaeger-client-go" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="3c8a">To add tracing to your gRPC microservices, we will use gRPC middleware to enable tracing on gRPC server and client. <code>grpc-ecosystem/go-grpc-middleware</code> has a great collection of interceptors, including support for OpenTracing provider-agnostic server-side and client-side interceptors.</p><p id="d180">The <code>grpc_opentracing</code> package exposes the opentracing interceptors that can be initialized with any <code>opentracing.Tracer</code> implementation. Here we initialize a gRPC server with chained unary and stream interceptors. Enabling this will create a root server<code>Span</code> and for each server-side gRPC request, the tracer will attach a <code>Span</code> to every RPC call defined in the service.</p><figure></figure><p id="b872">In order to enable tracing of both upstream and downstream requests of the gRPC service, the gRPC client must also be initialized with client-side <code>opentracing</code> interceptor, as shown in the following example:</p><figure></figure><p id="2928">The parent spans created by the gRPC middleware are injected to the go <code>context</code>, which enables powerful tracing support. The <code>opentracing</code> <a href="https://godoc.org/github.com/opentracing/opentracing-go#Span" rel="noopener ugc nofollow" target="_blank">go client</a> can be used to <a href="https://godoc.org/github.com/opentracing/opentracing-go#StartSpanFromContext" rel="noopener ugc nofollow" target="_blank">attach child spans to parent context</a> for more granular tracing, as well as control each span lifetime, add custom tags to traces, etc.</p><h2 id="97c9">Jaeger Agent</h2><p id="4f7f">The Jaeger agent is a daemon that receives spans from Jaeger clients over UDP, batches and forwards them to the collectors. The agent acts as a buffer to abstract out batch processing and routing from the clients.</p><p id="8538">Even though the agent was built to be used as a daemon, in a Kubernetes setup the agent can be configured to run as a sidecar container in the application Pod or as an independent <code>DaemonSet</code> .</p><p id="26ab">The following section discusses the pros and cons to each deployment strategy.</p><p id="ead3"><strong>Jaeger Agent as Sidecar</strong></p><p id="1e9f">A sidecar Jaeger agent is a container that sits in the same pod as your application container. The application, denoted as Jaeger service <code>myapp</code> will send Jaeger spans to the agent over <code>localhost</code> to port <code>6381</code>. These configurations are set via environment variables <code>JAEGER_SERVICE_NAME</code>, <code>JAEGER_AGENT_HOST</code> and <code>JAEGER_AGENT_PORT</code> in the client as mentioned earlier.</p><figure></figure><p id="4191">With this approach, each agent (and thus, each application) can be configured to send traces to different collectors (and thus, different backend storages).</p><p id="4da4">However, one of biggest drawbacks of this approach is tight-coupling of life-cycle of the agent and the application. Tracing is meant to give insights to your application during its life-time. More likely than not, the agent sidecar container is killed before main application container, and any/all the important traces during shutdown of the application service will go missing. The loss of these traces could be significant in understanding application life-cycle behavior of complex service interactions. <a href="https://github.com/jaegertracing/jaeger/issues/295" rel="noopener ugc nofollow" target="_blank">This Github issue</a> verifies the need for proper SIGTERM handling during shutdown.</p><p id="cf7b"><strong>Jaeger Agent as Daemonset</strong></p><p id="3384">The other approach is to run the agent as a daemon in each Node in the cluster, via a <code>DaemonSet</code> workload in Kubernetes. A <code>DaemonSet</code> workload ensures as Nodes are scaled, a copy of the DaemonSet Pod is scaled with it.</p><p id="cebc">In this scenario, each agent daemon is responsible for consuming traces from all running applications (with Jaeger client configured) scheduled in its Node. This is configured by setting the <code>JAEGER_AGENT_HOST</code> in the client to point to the IP of the agent in the Node. The Agent <code>DaemonSet</code> is configured with <code>hostNetwork: true</code> and <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods" rel="noopener ugc nofollow" target="_blank">appropriate DNS policy</a> so that the Pod uses the same IP as the host. Since the Agent port <code>6831</code> is exposed to accept jaeger.thrift messages over UDP, the daemon’s Pod is configured port is bound with <code>hostPort: 6831</code> as well.</p><figure></figure><p id="d4eb">One may be tempted (as I was), to front the <code>DaemonSet</code> with a Kubernetes service. The idea behind that was to not bind application traces to the single agent in the current Node. Using a service enables spreading the workload (spans) across all agents in the cluster. This in theory, reduces the chance of missing spans from application instances in the event of a failure of a single agent pod of the affected Node.</p><p id="3ccd">However, this will not work as your application scales and high load creates large spikes in the number of traces that need to be processed. Using a Kubernetes service means sending traces from client to agent over the network. Very soon, I started noticing high number of dropped spans. The client sends spans to agent over UDP thrift protocol and a large spike resulted in exceeding the UDP max packet size and thus dropped packets.</p><p id="9417">The solution is to allocate resources appropriately such that pods are scheduled by Kubernetes more evenly across the cluster. One could increase the queue-size of the client (set <code>JAEGER_REPORTER_MAX_QUEUE_SIZE</code> environment variable) to give enough buffer for spans while an agent fails over. It would be beneficial to increase the internal queue size of agent as well (set <code>processor.jaeger-binary.server-queue-size</code> value), so they are less likely to start dropping spans.</p><h2 id="28e3">Jaeger Collector Service</h2><p id="031e">The Jaeger Collector is responsible to receiving batches of spans from Jaeger Agent, run them through the processing pipeline and store them in specified storage backend. Spans are sent in <code>jaeger.thrift</code> format from Jaeger agent over over <code>TChannel</code> (TCP) protocol on port <code>14267</code>.</p><p id="45ea">The Jaeger collector is stateless and can be scaled to any number of instances on demand. Thus the collector can be fronted by a Kubernetes internal service (ClusterIP) that can load balance internal traffic from agents to the different collector instances.</p><figure></figure><h2 id="fe2c">Jaeger Query Service</h2><p id="5b22">The query service is the Jaeger server to back the UI. It is responsible for retrieving traces from storage and format them to display on the UI. Depending on usage of query service can have very tiny resource footprint.</p><p id="d0fc">Setup an ingress of internal Jaeger UI to point to backend query service.</p><figure></figure><h2 id="85be">Storage Configuration</h2><p id="9989">Jaeger supports both ElasticSearch and Cassandra as storage backend. Using ElasticSearch for storage enables to have a powerful monitoring infrastructure that ties tracing and logging together. Part of the collector processing pipeline is indexing the traces for its storage backend— this will enable traces to show up in your logging UI (Kibana for example) and also bind trace ID’s to your structured logging labels. You can set the storage type to ElasticSearch via environment variable on the <code>SPAN_STORAGE_TYPE</code> and configure the storage endpoint via configuration.</p><p id="1bd6">A Kubernetes <code>ConfigMap</code> is used to setup the storage configuration of some of the Jaeger components. For example, the storage backend type and endpoint for Jaeger Collector and Query service.</p><figure></figure><p id="87b1">As mentioned before, tracing is an important component of the monitoring infrastructure. That means, even components of your tracing infrastructure needs to be monitored as well.</p><p id="58b1">Jaeger <a href="https://www.jaegertracing.io/docs/monitoring/" rel="noopener ugc nofollow" target="_blank">exposes metrics in prometheus format</a> on specific ports for each component. If there are prometheus node exporters running (it should absolutely be) which are scraping for metrics on specific port — then map the metrics port of your Jaeger components to the port the node exporter is scraping metrics on.</p><p id="c33c">This can be done by updating the Jaeger services (agent, collector, query) to map their metrics port (5778, 14628, or 16686) to the port that the node exporters are expecting to scrape metrics (like 8888/8080 for example).</p><p id="04e9">Some important metrics to keep track of:</p><ul><li id="9c3e">Health of each component — memory usage:</li></ul><pre><span id="285d">sum(rate(container_memory_usage_bytes{container_name=~”^jaeger-.+”}[1m])) by (pod_name)</span></pre><ul><li id="09c8">Health of each component — CPU usage:</li></ul><pre><span id="ba0f">sum(rate(container_cpu_usage_seconds_total{container_name=~&#34;^jaeger-.+&#34;}[1m])) by (pod_name)</span></pre><ul><li id="dbfa">Batch failures by Jaeger Agent:</li></ul><pre><span id="8b8a">sum(rate(jaeger_agent_tc_reporter_jaeger_batches_failures[1m])) by (pod)</span></pre><ul><li id="fca3">Spans dropped by Collector:</li></ul><pre><span id="1acb">sum(rate(jaeger_collector_spans_dropped[1m])) by (pod)</span></pre><ul><li id="0131">Queue latency (p95) of Collector:</li></ul><pre><span id="7935">histogram_quantile(0.95, sum(rate(jaeger_collector_in_queue_latency_bucket[1m])) by (le, pod))</span></pre><p id="99b5">These metrics give important insights into how each component is performing and historical data should be used for optimal setup.</p></div></div>