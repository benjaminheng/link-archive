---
url: https://netflixtechblog.com/building-netflixs-distributed-tracing-infrastructure-bb856c319304
title: Building Netflix’s Distributed Tracing Infrastructure
archived_at: 2021-12-13T16:09:22.490443+08:00
---
<div id="readability-page-1" class="page"><div><div><p><a href="https://netflixtechblog.medium.com/?source=post_page-----bb856c319304-----------------------------------" rel="noopener follow"><img alt="Netflix Technology Blog" src="https://miro.medium.com/fit/c/96/96/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" width="48" height="48"/></a></p></div><p id="9f5a"><em>by</em> <a href="https://www.linkedin.com/in/maulikpandey/" rel="noopener ugc nofollow" target="_blank">Maulik Pandey</a></p><p id="aa99">Our Team — <a href="https://www.linkedin.com/in/kevin-lew-298155/" rel="noopener ugc nofollow" target="_blank">Kevin Lew</a>, <a href="https://www.linkedin.com/in/narayanan-a-0744291/" rel="noopener ugc nofollow" target="_blank">Narayanan Arunachalam</a>, <a href="https://www.linkedin.com/in/elizabethcarretto/" rel="noopener ugc nofollow" target="_blank">Elizabeth Carretto</a>, <a href="https://www.linkedin.com/in/dustin-haffner-55534aab/" rel="noopener ugc nofollow" target="_blank">Dustin Haffner</a>, Andrei Ushakov, <a href="https://www.linkedin.com/in/katzseth22202/" rel="noopener ugc nofollow" target="_blank">Seth Katz</a>, <a href="https://www.linkedin.com/in/greg-burrell-67ab273/" rel="noopener ugc nofollow" target="_blank">Greg Burrell</a>, <a href="https://www.linkedin.com/in/ramvaith/" rel="noopener ugc nofollow" target="_blank">Ram Vaithilingam</a>, <a href="https://www.linkedin.com/in/kerumai/" rel="noopener ugc nofollow" target="_blank">Mike Smith</a> and <a href="https://www.linkedin.com/in/maulikpandey/" rel="noopener ugc nofollow" target="_blank">Maulik Pandey</a></p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*szcAUd6S53CVSl6N" width="700" height="263" role="presentation"/></p></div></figure><p id="dcb8"><em>“</em><a href="https://twitter.com/Netflixhelps" rel="noopener ugc nofollow" target="_blank"><em>@Netflixhelps</em></a><em> Why doesn’t Tiger King play on my phone?” — a Netflix member via Twitter</em></p><p id="f7ce">This is an example of a question our on-call engineers need to answer to help resolve a member issue — which is difficult when troubleshooting distributed systems. Investigating a video streaming failure consists of inspecting all aspects of a member account. In our previous <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/edgar-solving-mysteries-faster-with-observability-e1a76302c71f">blog post</a> we introduced Edgar, our troubleshooting tool for streaming sessions. Now let’s look at how we designed the tracing infrastructure that powers Edgar.</p><p id="90cf">Prior to Edgar, our engineers had to sift through a mountain of metadata and logs pulled from various Netflix microservices in order to understand <em>a specific</em> streaming failure experienced by any of our members. Reconstructing a streaming session was a tedious and time consuming process that involved tracing all interactions (requests) between the Netflix app, our Content Delivery Network (CDN), and backend microservices. The process started with manual pull of member account information that was part of the session. The next step was to put all puzzle pieces together and hope the resulting picture would help resolve the member issue. We needed to increase engineering productivity via distributed request tracing.</p><p id="ff35">If we had an ID for each streaming session then distributed tracing could easily reconstruct session failure by providing service topology, retry and error tags, and latency measurements for all service calls. We could also get contextual information about the streaming session by joining relevant traces with account metadata and service logs. This insight led us to build Edgar: a distributed tracing infrastructure and user experience.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*KZD1JGqoK57fE2jY" width="700" height="468" srcset="https://miro.medium.com/max/552/0*KZD1JGqoK57fE2jY 276w, https://miro.medium.com/max/1104/0*KZD1JGqoK57fE2jY 552w, https://miro.medium.com/max/1280/0*KZD1JGqoK57fE2jY 640w, https://miro.medium.com/max/1400/0*KZD1JGqoK57fE2jY 700w" sizes="700px" role="presentation" data-old-src="https://miro.medium.com/max/60/0*KZD1JGqoK57fE2jY?q=20"/></p></div><figcaption>Figure 1. Troubleshooting a session in Edgar</figcaption></figure><p id="1e15">When we started building Edgar four years ago, there were very few open-source distributed tracing systems that satisfied our needs. Our tactical approach was to use Netflix-specific libraries for collecting traces from Java-based streaming services until open source tracer libraries matured. By 2017, open source projects like <a href="https://opentracing.io/" rel="noopener ugc nofollow" target="_blank">Open-Tracing</a> and <a href="https://github.com/openzipkin" rel="noopener ugc nofollow" target="_blank">Open-Zipkin</a> were mature enough for use in polyglot runtime environments at Netflix. We chose Open-Zipkin because it had better integrations with our Spring Boot based Java runtime environment. We use <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/open-sourcing-mantis-a-platform-for-building-cost-effective-realtime-operations-focused-5b8ff387813a">Mantis</a> for processing the stream of collected traces, and we use Cassandra for storing traces. Our distributed tracing infrastructure is grouped into three sections: tracer library instrumentation, stream processing, and storage. Traces collected from various microservices are ingested in a stream processing manner into the data store. The following sections describe our journey in building these components.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/1*YhKS1wzlN7CR2iVT18VnbA.jpeg" width="700" height="525" srcset="https://miro.medium.com/max/552/1*YhKS1wzlN7CR2iVT18VnbA.jpeg 276w, https://miro.medium.com/max/1104/1*YhKS1wzlN7CR2iVT18VnbA.jpeg 552w, https://miro.medium.com/max/1280/1*YhKS1wzlN7CR2iVT18VnbA.jpeg 640w, https://miro.medium.com/max/1400/1*YhKS1wzlN7CR2iVT18VnbA.jpeg 700w" sizes="700px" role="presentation" data-old-src="https://miro.medium.com/max/60/1*YhKS1wzlN7CR2iVT18VnbA.jpeg?q=20"/></p></div></figure><p id="ca7c">That is the first question our engineering teams asked us when integrating the tracer library. It is an important question because tracer libraries intercept all requests flowing through mission-critical streaming services. Safe integration and deployment of tracer libraries in our polyglot runtime environments was our top priority. We earned the trust of our engineers by developing empathy for their operational burden and by focusing on providing efficient tracer library integrations in runtime environments.</p><p id="8794">Distributed tracing relies on propagating context for local interprocess calls (IPC) and client calls to remote microservices for any arbitrary request. Passing the request context captures causal relationships between microservices during runtime. We adopted Open-Zipkin’s <a href="https://github.com/openzipkin/b3-propagation" rel="noopener ugc nofollow" target="_blank">B3 HTTP header</a> based context propagation mechanism. We ensure that context propagation headers are correctly passed between microservices across a variety of our “<a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/how-we-build-code-at-netflix-c5d9bd727f15">paved road</a>” Java and Node runtime environments, which include both older environments with legacy codebases and newer environments such as Spring Boot. We execute the<em> </em><a href="https://jobs.netflix.com/culture" rel="noopener ugc nofollow" target="_blank"><em>Freedom &amp; Responsibility </em>principle of our culture</a> in supporting tracer libraries for environments like Python, NodeJS, and Ruby on Rails that are not part of the “paved road” developer experience. Our <a href="https://jobs.netflix.com/culture" rel="noopener ugc nofollow" target="_blank"><em>loosely coupled but highly aligned</em></a> engineering teams have the freedom to choose an appropriate tracer library for their runtime environment and have the responsibility to ensure correct context propagation and integration of network call interceptors.</p><p id="2ae2">Our runtime environment integrations inject infrastructure tags like service name, auto-scaling group (ASG), and container instance identifiers. Edgar uses this infrastructure tagging schema to query and join traces with log data for troubleshooting streaming sessions. Additionally, it became easy to provide deep links to different monitoring and deployment systems in Edgar due to consistent tagging. With runtime environment integrations in place, we had to set an appropriate trace data sampling policy for building a troubleshooting experience.</p><p id="6a68">This was the most important question we considered when building our infrastructure because data sampling policy dictates the amount of traces that are recorded, transported, and stored. A lenient trace data sampling policy generates a large number of traces in each service container and can lead to degraded performance of streaming services as more CPU, memory, and network resources are consumed by the tracer library. An additional implication of a lenient sampling policy is the need for scalable stream processing and storage infrastructure fleets to handle increased data volume.</p><p id="f88d">We knew that a heavily sampled trace dataset is not reliable for troubleshooting because there is no guarantee that the request you want is in the gathered samples. We needed a thoughtful approach for collecting all traces in the streaming microservices while keeping low operational complexity of running our infrastructure.</p><p id="48eb">Most distributed tracing systems enforce sampling policy at the request ingestion point in a microservice call graph. We took a <a href="https://github.com/openzipkin-contrib/zipkin-secondary-sampling/blob/master/docs/design.md" rel="noopener ugc nofollow" target="_blank">hybrid head-based sampling approach</a> that allows for recording 100% of traces for a specific and configurable set of requests, while continuing to randomly sample traffic per the policy set at ingestion point. This flexibility allows tracer libraries to record 100% traces in our mission-critical streaming microservices while collecting minimal traces from auxiliary systems like offline batch data processing. Our engineering teams tuned their services for performance after factoring in increased resource utilization due to tracing. The next challenge was to stream large amounts of traces via a scalable data processing platform.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*NxgzI5VQyp3NIE_Y" width="700" height="471" srcset="https://miro.medium.com/max/552/0*NxgzI5VQyp3NIE_Y 276w, https://miro.medium.com/max/1104/0*NxgzI5VQyp3NIE_Y 552w, https://miro.medium.com/max/1280/0*NxgzI5VQyp3NIE_Y 640w, https://miro.medium.com/max/1400/0*NxgzI5VQyp3NIE_Y 700w" sizes="700px" role="presentation" data-old-src="https://miro.medium.com/max/60/0*NxgzI5VQyp3NIE_Y?q=20"/></p></div></figure><p id="2c74"><a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/open-sourcing-mantis-a-platform-for-building-cost-effective-realtime-operations-focused-5b8ff387813a">Mantis</a> is our go-to platform for processing operational data at Netflix. We chose Mantis as our backbone to transport and process large volumes of trace data because we needed a backpressure-aware, scalable stream processing system. Our trace data collection agent transports traces to Mantis job cluster via the <a href="https://netflix.github.io/mantis/internals/mantis-publish/" rel="noopener ugc nofollow" target="_blank">Mantis Publish library</a>. We buffer spans for a time period in order to collect all spans for a trace in the first job. A second job taps the data feed from the first job, does tail sampling of data and writes traces to the storage system. This setup of <a href="https://netflix.github.io/mantis/getting-started/concepts/#job-chaining" rel="noopener ugc nofollow" target="_blank">chained Mantis jobs</a> allows us to scale each data processing component independently. An additional advantage of using Mantis is the ability to perform real-time ad-hoc data exploration in <a href="https://www.youtube.com/watch?v=uODxUJ5Jwis" rel="noopener ugc nofollow" target="_blank">Raven</a> using the <a href="https://netflix.github.io/mantis/reference/mql/" rel="noopener ugc nofollow" target="_blank">Mantis Query Language (MQL)</a>. However, having a scalable stream processing platform doesn’t help much if you can’t store data in a cost efficient manner.</p><p id="79af">We started with Elasticsearch as our data store due to its flexible data model and querying capabilities. As we onboarded more streaming services, the trace data volume started increasing exponentially. The increased operational burden of scaling ElasticSearch clusters due to high data write rate became painful for us. The data read queries took an increasingly longer time to finish because ElasticSearch clusters were using heavy compute resources for creating indexes on ingested traces. The high data ingestion rate eventually degraded both read and write operations. We solved this by migrating to <a href="https://netflixtechblog.com/tagged/cassandra" rel="noopener ugc nofollow" target="_blank">Cassandra</a> as our data store for handling high data ingestion rates. Using simple lookup indices in Cassandra gives us the ability to maintain acceptable read latencies while doing heavy writes.</p><p id="7d4a">In theory, scaling up horizontally would allow us to handle higher write rates and retain larger amounts of data in Cassandra clusters. This implies that the cost of storing traces grows linearly to the amount of data being stored. We needed to ensure storage cost growth was <em>sub-linear</em> to the amount of data being stored. In pursuit of this goal, we outlined following storage optimization strategies:</p><ol><li id="733f">Use cheaper <a href="https://aws.amazon.com/ebs/" rel="noopener ugc nofollow" target="_blank">Elastic Block Store</a> (EBS) volumes instead of SSD instance stores in EC2.</li><li id="a82e">Employ better compression technique to reduce trace data size.</li><li id="9635">Store only relevant and interesting traces by using simple rules-based filters.</li></ol><p id="115f">We were adding new Cassandra nodes whenever the EC2 SSD instance stores of existing nodes reached maximum storage capacity. The use of a cheaper EBS Elastic volume instead of an SSD instance store was an attractive option because AWS allows dynamic increase in EBS volume size without re-provisioning the EC2 node. This allowed us to increase total storage capacity without adding a new Cassandra node to the existing cluster. In 2019 our stunning colleagues in the Cloud Database Engineering (CDE) team benchmarked EBS performance for our use case and migrated existing clusters to use EBS Elastic volumes. By optimizing the Time Window Compaction Strategy (TWCS) parameters, they reduced the disk write and merge operations of Cassandra SSTable files, thereby reducing the EBS I/O rate. This optimization helped us reduce the data replication network traffic amongst the cluster nodes because SSTable files were created less often than in our previous configuration. Additionally, by enabling Zstd block compression on Cassandra data files, the size of our trace data files was reduced by half. With these optimized Cassandra clusters in place, it now costs us 71% less to operate clusters and we could store 35x more data than our previous configuration.</p><p id="0759">We observed that Edgar users explored less than 1% of collected traces. This insight leads us to believe that we can reduce write pressure and retain more data in the storage system if we drop traces that users will not care about. We currently use a simple rule based filter in our Storage Mantis job that retains interesting traces for very rarely looked service call paths in Edgar. The filter qualifies a trace as an interesting data point by inspecting all buffered spans of a trace for warnings, errors, and retry tags. This tail-based sampling approach reduced the trace data volume by 20% without impacting user experience. There is an opportunity to use machine learning based classification techniques to further reduce trace data volume.</p><p id="db41">While we have made substantial progress, we are now at another inflection point in building our trace data storage system. Onboarding new user experiences on Edgar could require us to store 10x the amount of current data volume. As a result, we are currently experimenting with a tiered storage approach for a new data gateway. This data gateway provides a querying interface that abstracts the complexity of reading and writing data from tiered data stores. Additionally, the data gateway routes ingested data to the Cassandra cluster and transfers compacted data files from Cassandra cluster to S3. We plan to retain the last few hours worth of data in Cassandra clusters and keep the rest in S3 buckets for long term retention of traces.</p><figure><div><p><img alt="" src="https://miro.medium.com/max/1128/1*5qTy7E2-kbiTQjxxcY9EAg.png" width="564" height="248" srcset="https://miro.medium.com/max/552/1*5qTy7E2-kbiTQjxxcY9EAg.png 276w, https://miro.medium.com/max/1104/1*5qTy7E2-kbiTQjxxcY9EAg.png 552w, https://miro.medium.com/max/1128/1*5qTy7E2-kbiTQjxxcY9EAg.png 564w" sizes="564px" role="presentation" data-old-src="https://miro.medium.com/max/60/1*5qTy7E2-kbiTQjxxcY9EAg.png?q=20"/></p></div><figcaption>Table 1. Timeline of Storage Optimizations</figcaption></figure><p id="7e56">In addition to powering Edgar, trace data is used for the following use cases:</p><p id="1367"><strong>Application Health Monitoring</strong></p><p id="598f">Trace data is a key signal used by <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/telltale-netflix-application-monitoring-simplified-5c08bfa780ba">Telltale</a> in monitoring macro level application health at Netflix. Telltale uses the causal information from traces to infer microservice topology and correlate traces with time series data from <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a">Atlas</a>. This approach paints a richer observability portrait of application health.</p><p id="be64"><strong>Resiliency Engineering</strong></p><p id="0ee8">Our chaos engineering team uses traces to verify that failures are correctly injected while our engineers <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/chap-chaos-automation-platform-53e6d528371f">stress test</a> their microservices via <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/fit-failure-injection-testing-35d8e2a9bb2">Failure Injection Testing</a> (FIT) platform.</p><p id="1d23"><strong>Regional Evacuation</strong></p><p id="e547">The Demand Engineering team leverages tracing to <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/evolving-regional-evacuation-69e6cc1d24c6">improve the correctness of prescaling</a> during regional evacuations. Traces provide visibility into the types of devices interacting with microservices such that changes in demand for these services can be better accounted for when an AWS region is evacuated.</p><p id="c1e0"><strong>Estimate infrastructure cost of running an A/B test</strong></p><p id="0b79">The Data Science and Product team factors in the costs of running <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15">A/B tests</a> on microservices by analyzing traces that have relevant A/B test names as tags.</p><p id="ecce">The scope and complexity of our software systems continue to increase as Netflix grows. We will focus on following areas for extending Edgar:</p><ul><li id="6aa9">Provide a great developer experience for collecting traces across all runtime environments. With an easy way to to try out distributed tracing, we hope that more engineers instrument their services with traces and provide additional context for each request by tagging relevant metadata.</li><li id="3178">Enhance our analytics capability for querying trace data to enable power users at Netflix in building their own dashboards and systems for narrowly focused use cases.</li><li id="2022">Build abstractions that correlate data from metrics, logging, and tracing systems to provide additional contextual information for troubleshooting.</li></ul><p id="15a1">As we progress in building distributed tracing infrastructure, our engineers continue to rely on Edgar for troubleshooting streaming issues like “<em>Why doesn’t Tiger King play on my phone?”</em>. Our distributed tracing infrastructure helps in ensuring that Netflix members continue to enjoy a must-watch show like <a href="https://www.netflix.com/title/81115994" rel="noopener ugc nofollow" target="_blank">Tiger King</a>!</p><p id="8f65"><em>We are looking for stunning colleagues to join us on this journey of building distributed tracing infrastructure. If you are passionate about Observability then come </em><a href="https://jobs.netflix.com/search?q=observability" rel="noopener ugc nofollow" target="_blank"><em>talk to us</em></a><em>.</em></p></div></div>