---
url: https://www.interdb.jp/pg/pgsql11.html
title: 'The Internals of PostgreSQL : Chapter 11 Streaming Replication'
archived_at: 2022-01-29T17:14:24.782852+08:00
---
<div id="readability-page-1" class="page"><div>
 <div>




<p>
The synchronous streaming replication was implemented in version 9.1. It is a so-called single-master-multi-slaves type replication, and those two terms – master and slave(s) – are usually referred to as <b>primary</b> and <b>standby(s)</b> respectively in PostgreSQL.
</p>

<p>
This native replication feature is based on the log shipping, one of the general replication techniques, in which a primary server continues to send <b>WAL data</b> and then, each standby server replays the received data immediately.
</p>

<p>
This chapter covers the following topics focusing on how streaming replication works:
</p>

<ul>
 <li>How Streaming Replication starts up</li>
 <li>How the data are transferred between primary and standby servers
 </li><li>How primary server manages multiple standby servers</li>
 <li>How primary server detects failures of standby servers</li>
</ul>

<br/>
<div><p>
Though the first replication feature, only for asynchronous replication, had implemented in version 9.0, it had replaced with the new implementation (currently in use) for synchronous replication in 9.1.
</p></div>
<br/>




  </div>
<p><h2><a id="_11.1." name="_11.1."></a>11.1. Starting the Streaming Replication</h2></p>
 <div>



<p>
In Streaming Replication, three kinds of processes work cooperatively. A <b>walsender</b> process on the primary server sends WAL data to standby server; and then, a <b>walreceiver</b> and a <b>startup</b> processes on standby server receives and replays these data. A walsender and a walreceiver communicate using a single TCP connection.
</p>
<p>
In this section, we will explore the start-up sequence of the streaming replication to understand how those processes are started and how the connection between them is established. Figure 11.1 shows the startup sequence diagram of streaming replication:
</p>


<p><b>Fig. 11.1. SR startup sequence.</b></p><div>
<figure>
<img alt="Fig. 11.1. SR startup sequence." src="https://www.interdb.jp/pg/img/fig-11-01.png"/>
</figure>
  </div>



<ul>
<li>
(1) Start primary and standby servers.
</li>
<li>
(2) The standby server starts a startup process.
</li>
<li>
(3) The standby server starts a walreceiver process.
</li>
<li>
(4) The walreceiver sends a connection request to the primary server. If the primary server is not running, the walreceiver sends these requests periodically.
</li>
<li>
(5) When the primary server receives a connection request, it starts a walsender process and a TCP connection is established between the walsender and walreceiver.
</li>
<li>
(6) The walreceiver sends the latest LSN of standby&#39;s database cluster. In general, this phase is known as <b>handshaking</b> in the field of information technology.
</li>
<li>
(7) If the standby&#39;s latest LSN is less than the primary&#39;s latest LSN (Standby&#39;s LSN &lt; Primary&#39;s LSN), the walsender sends WAL data from the former LSN to the latter LSN. Such WAL data are provided by WAL segments stored in the primary&#39;s pg_xlog subdirectory (in version 10 or later, pg_wal subdirectory). Then, the standby server replays the received WAL data. In this phase, the standby catches up with the primary, so it is called <b>catch-up</b>.
</li>
<li>
(8) Streaming Replication begins to work.
</li>
</ul>

<p>
Each walsender process keeps a state appropriate for the working phase of connected walreceiver or any application (Note that it is not the state of walreceiver or application connected to the walsender.) The following are the possible states of it:
</p>

<ul>
<li>start-up – From starting the walsender to the end of handshaking. See Figs. 11.1(5)–(6).</li>
<li>catch-up – During the catch-up phase. See Fig. 11.1(7).</li>
<li>streaming – While Streaming Replication is working. See Fig. 11.1(8).</li>
<li>backup – During sending the files of the whole database cluster for backup tools such as <i>pg_basebackup</i> utility.</li>
</ul>

<p>
The <i>pg_stat_replication</i> view shows the state of all running walsenders. An example is shown below:
</p>

<pre>testdb=# SELECT application_name,state FROM pg_stat_replication;
 application_name |   state
------------------+-----------
 standby1         | streaming
 standby2         | streaming
 pg_basebackup    | backup
(3 rows)
</pre>

<p>
As shown in the above result, two walsenders are running to send WAL data for the connected standby servers, and another one is running to send all files of the database cluster for <i><a href="http://www.postgresql.org/docs/current/static/app-pgbasebackup.html" target="_blank" rel="noopener noreferrer">pg_basebackup</a></i> utility.
</p>

<br/>
<div>
  <p><i>  <a id="_repslot" name="_repslot"></a>What will happen if a standby server restarts after a long time in the stopped condition?</i>
  </p>
  <div>
<p>
In version 9.3 or earlier, if the primary&#39;s WAL segments required by the standby server have already been recycled, the standby cannot catch up with the primary server. There is no reliable solution for this problem, but only to set a large value to the configuration parameter <a href="http://www.postgresql.org/docs/current/static/runtime-config-replication.html#GUC-WAL-KEEP-SEGMENTS" target="_blank" rel="noopener noreferrer"><i>wal_keep_segments</i></a> to reduce the possibility of the occurrence. It&#39;s a stopgap solution.
</p>
<p>
In version 9.4 or later, this problem can be prevented by using <i>replication slot</i>. The replication slot is a feature that expands the flexibility of the WAL data sending, mainly for the <i>logical replication</i>, which also provides the solution to this problem – the WAL segment files which contain unsent data under the <i>pg_xlog</i> (or <i>pg_wal</i> if version 10 or later) can be kept in the replication slot by pausing recycling process. Refer the <a href="http://www.postgresql.org/docs/current/static/warm-standby.html#STREAMING-REPLICATION-SLOTS" target="_blank" rel="noopener noreferrer">official document</a> for detail.
</p>
  </div>
</div>
<br/>




  </div>
<p><h2><a id="_11.2." name="_11.2."></a>11.2. How to Conduct Streaming Replication</h2></p>
 <div>


<p>
Streaming replication has two aspects: log shipping and database synchronization. Log shipping is obviously one of those aspects since the streaming replication is based on it – the primary server sends WAL data to the connected standby servers whenever the writing of them occurs. Database synchronization is required for synchronous replication – the primary server communicates with each multiple-standby server to synchronize its database clusters.
</p>
<p>
To accurately understand how streaming replication works, we should explore how one primary server manages multiple standby servers. To make it rather simple, the special case (i.e. single-primary single-standby system) is described in this section, while the general case (single-primary multi-standbys system) will be described in the next section.
</p>



  </div>
<p><h3><a id="_11.2.1." name="_11.2.1."></a>11.2.1. Communication Between a Primary and a Synchronous Standby</h3></p>
 <div>



<p>
Assume that the standby server is in the synchronous replication mode, but the configuration parameter <i>hot-standby</i> is disabled and <i>wal_level</i> is &#39;<i>archive</i>&#39;. The main parameter of the primary server is shown below:
</p>

<pre>synchronous_standby_names = &#39;standby1&#39;
hot_standby = off
wal_level = archive
</pre>

<p>
Additionally, among the three triggers to write the WAL data mentioned in <a href="https://www.interdb.jp/pg/pgsql09.html#_9.5.">Section 9.5</a>, we focus on the transaction commits here.
</p>

<p>
Suppose that one backend process on the primary server issues a simple INSERT statement in autocommit mode. The backend starts a transaction, issues an INSERT statement, and then commits the transaction immediately. Let&#39;s explore further how this commit action will be completed. See the following sequence diagram in Fig. 11.2:
</p>




<p><b>Fig. 11.2. Streaming Replication&#39;s communication sequence diagram.</b></p><div>

<figure>
<img alt="Fig. 11.2. Streaming Replication&#39;s communication sequence diagram." src="https://www.interdb.jp/pg/img/fig-11-02.png"/>
</figure>
  </div>


<ul>
<li>
(1) The backend process writes and flushes WAL data to a WAL segment file by executing the functions <i>XLogInsert()</i> and <i>XLogFlush()</i>.
</li>
<li>
(2) The walsender process sends the WAL data written into the WAL segment to the walreceiver process.
</li>
<li>
(3) After sending the WAL data, the backend process continues to wait for an ACK response from the standby server. More precisely, the backend process gets a latch by executing the internal function <i>SyncRepWaitForLSN()</i>, and waits for it to be released.
</li>
<li>
(4) The walreceiver on standby server writes the received WAL data into the standby&#39;s WAL segment using the <i>write()</i> system call, and returns an ACK response to the walsender.
</li>
<li>
(5) The walreceiver flushes the WAL data to the WAL segment using the system call such as <i>fsync()</i>, returns another ACK response to the walsender, and informs the startup process about WAL data updated.
</li>
<li>
(6) The startup process replays the WAL data, which has been written to the WAL segment.
</li>
<li>
(7) The walsender releases the latch of the backend process on receiving the ACK response from the walreceiver, and then, backend process&#39;s commit or abort action will be completed. The timing for latch-release depends on the parameter <i>synchronous_commit</i>. It is <i>&#39;on&#39;</i> (default), the latch is released when the ACK of step (5) received, whereas it is <i>&#39;remote_write&#39;</i>, when the ACK of step (4) received.
</li>
</ul>

<br/>
<div><p>
If the configuration parameter <i>wal_level</i> is <i>&#39;hot_standby&#39;</i> or <i>&#39;logical&#39;</i>, PostgreSQL writes a WAL record regarding the hot standby feature, following the records of a commit or abort action. (In this example, PostgreSQL does not write that record because it’s <i>&#39;archive&#39;</i>.)
</p></div>


<p>
Each ACK response informs the primary server of the internal information of standby server. It contains four items below:
</p>
 
<ul>
<li>LSN location where the latest WAL data has been written.</li>
<li>LSN location where the latest WAL data has been flushed.</li>
<li>LSN location where the latest WAL data has been replayed in the startup process.</li>
<li>The timestamp when this response has be sent.</li>
</ul>

<p>
Walreceiver returns ACK responses not only when WAL data have been written and flushed, but also periodically as the heartbeat of standby server. The primary server therefore always grasps the status of all connected standby servers.
</p>
<p>
By issuing the queries as shown below, the LSN related information of the connected standby servers can be displayed.
</p>

<pre>testdb=# SELECT application_name AS host,
        write_location AS write_LSN, flush_location AS flush_LSN, 
        replay_location AS replay_LSN FROM pg_stat_replication;

   host   | write_lsn | flush_lsn | replay_lsn 
----------+-----------+-----------+------------
 standby1 | 0/5000280 | 0/5000280 | 0/5000280
 standby2 | 0/5000280 | 0/5000280 | 0/5000280
(2 rows)
</pre>

<br/>
<div><p>
The heartbeat&#39;s interval is set to the parameter <i>wal_receiver_status_interval</i>, which is 10 seconds by default.
</p></div>
<br/>



  </div>
<p><h3><a id="_11.2.2." name="_11.2.2."></a>11.2.2. Behavior When a Failure Occurs</h3></p>
 <div>



<p>
In this subsection, descriptions are made on how the primary server behaves when the synchronous standby server has failed, and how to deal with the situation.
</p>

<p>
Even if the synchronous standby server has failed and is no longer able to return an ACK response, the primary server continues to wait for responses forever. So, the running transactions cannot commit and subsequent query processing cannot be started. In other words, all of the primary server operations are practically stopped. (Streaming replication does not support a function to revert automatically to asynchronous-mode by timing out.)
</p>

<p>
There are two ways to avoid such situation. One of them is to use multiple standby servers to increase the system availability, and the other is to switch from synchronous to <i>asynchronous</i> mode by performing the following steps manually.
</p>

<ul>
<li>
(1) Set empty string to the parameter <i>synchronous_standby_names</i>.
</li>
<pre>synchronous_standby_names = &#39;&#39;
</pre>
<li>
(2) Execute the pg_ctl command with <i>reload</i> option.
</li>
<pre>postgres&gt; pg_ctl -D $PGDATA reload
</pre>
</ul>

<p>
The above procedure does not affect the connected clients. The primary server continues the transaction processing as well as all sessions between clients and the respective backend processes are kept.
</p>




  </div>
<p><h2><a id="_11.3." name="_11.3."></a>11.3. Managing Multiple-Standby Servers</h2></p>
 <div>
  <p>
In this section, the way streaming replication works with multiple standby servers is described.
</p>
 </div>
<p><h3><a id="_11.3.1." name="_11.3.1."></a>11.3.1. sync_priority and sync_state </h3></p>
 <div>


<p>
Primary server gives <i>sync_priority</i> and <i>sync_state</i> to all managed standby servers, and treats each standby server depending on its respective values. (The primary server gives those values even if it manages just one standby server; haven’t mentioned this in the previous section.)
</p>

<p>
<i>sync_priority</i> indicates the priority of standby server in synchronous-mode and is a fixed value. The smaller value shows the higher priority, while 0 is the special value that means ‘in asynchronous-mode’. Priorities of standby servers are given in the order listed in the primary&#39;s configuration parameter <i>synchronous_standby_names</i>. For example, in the following configuration, priorities of standby1 and standby2 are 1 and 2, respectively.
</p>

<pre>synchronous_standby_names = &#39;standby1, standby2&#39;
</pre>

<p>
(Standby servers not listed on this parameter are in asynchronous-mode, and their priority is 0.)
</p>

<p>
<i>sync_state</i> is the state of the standby server. It is variable according to the running status of all standby servers and the individual priority. The followings are the possible states:
</p>

<ul>
<li>
<b>Sync</b> is the state of synchronous-standby server of the highest priority among all working standbys (except asynchronous-servers).
</li>
<li>
<b>Potential</b> is the state of spare synchronous-standby server of the second or lower priority among the all working standbys (except asynchronous-servers). If the synchronous-standby has failed, it will be replaced with the highest priority standby within the potential ones.
</li>
<li>
<b>Async</b> is the state of asynchronous-standby server, and this state is fixed. The primary server treats asynchronous-standbys in the same way as potential standbys except that their <i>sync_state</i> never be <i>&#39;sync&#39;</i> or <i>&#39;potential&#39;</i>.
</li>
</ul>

<p>
The priority and the state of the standby servers can be shown by issuing the following query:
</p>

<pre>testdb=# SELECT application_name AS host, 
         sync_priority, sync_state FROM pg_stat_replication;
   host   | sync_priority | sync_state
----------+---------------+------------
 standby1 |             1 | sync
 standby2 |             2 | potential
(2 rows)
</pre>


<br/>
<div><p>
Several developers are recently trying to implement ‘multiple synchronous-standby’. See <a href="https://commitfest.postgresql.org/6/293/" target="_blank" rel="noopener noreferrer">here</a> for details.
</p></div>
<br/>




  </div>
<p><h3><a id="_11.3.2." name="_11.3.2."></a>11.3.2. How the Primary Manages Multiple-standbys</h3></p>
 <div>


<p>
The primary server waits for ACK responses from the synchronous standby server alone. In other words, the primary server confirms only synchronous standby&#39;s writing and flushing of WAL data. Streaming replication, therefore, ensures that only synchronous standby is in the consistent and synchronous state with the primary.
</p>
<p>
Figure 11.3 shows the case in which the ACK response of potential standby has been returned earlier than that of the primary standby. There, the primary server does not complete the commit action of the current transaction, and continues to wait for the primary&#39;s ACK response. And then, when the primary&#39;s response is received, the backend process releases the latch and completes the current transaction processing.
</p>



<p><b>Fig. 11.3. Managing multiple standby servers.</b></p><div>
<figure>
<img alt="Fig. 11.3. Managing multiple standby servers." src="https://www.interdb.jp/pg/img/fig-11-03.png"/>
</figure>
       <div id="small">
<hr/><p>
The sync_state of standby1 and standby2 are <i>&#39;sync&#39;</i> and <i>&#39;potential&#39;</i> respectively.
(1) In spite of receiving an ACK response from the potential standby server, 
the primary&#39;s backend process continues to wait for an ACK response from the synchronous-standby server.
(2) The primary&#39;s backend process releases the latch, completes the current transaction processing.
            </p></div>
  </div>


<p>
In the opposite case (i.e. the primary&#39;s ACK response has been returned earlier than the potential&#39;s one), the primary server immediately completes the commit action of the current transaction without ensuring if the potential standby writes and flushes WAL data or not.
</p>


  </div>
<p><h3><a id="_11.3.3." name="_11.3.3."></a>11.3.3. Behavior When a Failure Occurs</h3></p>
 <div>



<p>
Once again, see how the primary server behaves when the standby server has failed.
</p>
<p>
When either a potential or an asynchronous standby server has failed, the primary server terminates the walsender process connected to the failed standby and continues all processing. In other words, transaction processing of the primary server would not be affected by the failure of either type of standby server.
</p>
<p>
When a synchronous standby server has failed, the primary server terminates the walsender process connected to the failed standby, and replaces synchronous standby with the highest priority potential standby. See Fig. 11.4. In contrast to the failure described above, query processing on the primary server will be paused from the point of failure to the replacement of synchronous standby. (Therefore, failure detection of standby server is a very important function to increase availability of replication system. Failure detection will be described in the next section.)
</p>


<p><b>Fig. 11.4. Replacing of synchronous standby server.</b></p><div>
<figure>
<img alt="Fig. 11.4. Replacing of synchronous standby server." src="https://www.interdb.jp/pg/img/fig-11-04.png"/>
</figure>
  </div>



<p>
In any case, if one or more standby server shall be running in syncrhonous-mode, the primary server keeps only one synchronous standby server at all times, and the synchronous standby server is always in a consistent and synchronous state with the primary.
</p>


  </div>
<p><h2><a id="_11.4." name="_11.4."></a>11.4. Detecting Failures of Standby Servers</h2></p>
 <div>


<p>
Streaming replication uses two common failure detection procedures that will not require any special hardware at all.
</p>

<ol>
<li>Failure detection of standby server process</li>
<p>
When a connection drop between walsender and walreceiver has been detected, the primary server <i>immediately</i> determines that the standby server or walreceiver process is faulty. When a low level network function returns an error by failing to write or to read the socket interface of walreceiver, the primary also <i>immediately</i> determines its failure.
</p>

<li>Failure detection of hardware and networks</li>
<p>
If a walreceiver returns nothing within the time set for the parameter <i>wal_sender_timeout</i> (default 60 seconds), the primary server determines that the standby server is faulty. In contrast to the failure described above, it takes a certain amount of time – maximum is <i>wal_sender_timeout</i> seconds – to confirm the standby&#39;s death on the primary server even if a standby server is no longer able to send any response by some failures (e.g. standby server&#39;s hardware failure, network failure, and so on).
</p>
</ol>

<p>
Depending on the types of failures, it can usually be detected immediately after a failure occurs, while sometimes there might be a time lag between the occurrence of failure and the detection of it. In particular, if a latter type of failure occurs in synchronous standby server, all transaction processing on the primary server will be stopped until detecting the failure of standby, even though multiple potential standby servers may have been working.
</p>

<br/>
<div><p>
The parameter <i>wal_sender_timeout</i> was called as <i>replication_timeout</i> in version 9.2 or earlier.
</p></div>
<br/>



</div>
            </div></div>